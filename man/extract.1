.TH extract 1 "October 16, 2011" "version 1.0" "USER COMMANDS"
.SH NAME
extract \- extract articles from a Wikipedia XML database.

.SH SYNOPSIS
.B extract [-d directory]

.SH DESCRIPTION 
The extract utility reads a Wikipedia XML database of articles from
standard input and extracts MediaWiki markup for each article. The
output is printed to standard output, or, if the \-d parameter is
specified, to separate files in a directory.

.PP
extract can work with very large databases (tens of gigabytes) without
running our of memory.

.SH OPTIONS
.TP
\-n NUMBER
specify the number of words per ngram: 1 for unigrams, 2 for bigrams etc.

.TP
\-m LIMIT 
specify the maximum amount of memory (bytes) that ngrams can use for
processing. LIMIT has to be an integer optionally followed by one of the
following multiplier suffixes: b 512, k 1024, m 1024*1024, g 1024*1024*1024.
The default value is 500M. Very small values may have negative effect
on performance.

The value of LIMIT does not affect the maximum size of datasets that ngrams 
can process: it is possible, though not recommended, to process gigabytes
of data in a few megabytes of memory.

The actual memory used by ngrams will be equal to c*LIMIT, where c
is a small system-dependent constant. Hence it is possible for ngrams
to run out of memory even if LIMIT is less than the total amount of
available RAM. In such cases, decrease LIMIT.

.TP
\-v
turns on verbose mode. Intended for debugging only.
